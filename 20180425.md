4/25

강화학습 개요


Lecture 4: Q-learning exploit&exploration and discounted reward
http://hunkim.github.io/ml/RL/rl04.pdf

exploit : 현재 경험 기준으로 greedy하게 선택
exploration: 현재 경험 외에 새로운 시도를 하게 함, 탐험, local optima에 빠지지 않도록 하기 위함

예) 맛집 방문.... 기존 경험치 기준으로 방문, 가끔 새로운 장소도 도전

add noise 방식, e-greedy 방식 등이 있음 

Q함수의 값 업데이트 방식은 그냥 새로운 예상치 값으로 변경함!!

Q(s,a) = r + max Q(s',a')

Q(s,a) = r + discount * max Q(s',a')



Lecture 5: Q-learning in non-deterministic world 비디오  강의 슬라이드 
http://hunkim.github.io/ml/RL/rl05.pdf

deterministic vs nondeterministic 

deterministic : agent가 수행한 action에 따라 state의 변화가 고정적임
nondeterministic : agent가 수행한 action에 따라 state의 변화가 확률적임 (일반적인 문제들의 경우 변화되는 확률을 잘 모름)

기존 경험치의 값을 유지하지 않고, 바로 업데이트하는 방식으으로는 nondeterministic 환경의 학습이 잘 이뤄지지 않음
> 약간의 오차만으로도 전체 값이 변경됨
Q함수의 값을 update하는 방식이 다름, learning rate 기반으로 기존 Q함수의 값을 유지하면서 조금씩 반영하도록 함
Q(s,a) = (1-a)*Q(s,a) + a*( r + discount * max Q(s',a'))


