4/18일(수)

.김성훈교수님 모두를 위한 RL 동영상 슬라이드 1장 ~ 3장

1장
http://hunkim.github.io/ml/RL/rl01.pdf

reinforcement(강화) : 어떤 행동의 강화(칭찬,혼내기), 강아지를 훈련할 때도 행동에 대해 보상하여 좋은 행동을 강화함

학습의 고유한 특성은 과거의 경험으로 부터 배워나가는 것임, 과거에 어떤 경험을 했는지가 현재의 내가 어떤 행동을 할지에 영향

주요 키워드:
환경(Environment) 를 통해서 관측, 상태를 얻고 해당 상태에 맞는 행동을 수행하고 보상을 얻는 사이클의 순환임
agent : 환경안에서 학습이 되어야 할 대상 (미로안의 생쥐~, 환경은 미로, 보상은 치즈~)

관측(Observation), 상태(state) : 다음 행동을 하기 위해 판단의 근거가 되는 정보, 예) 현재 생쥐의 위치
행동(action) : agent가 할 수 있는 행동  예) 생쥐가 직진, 왼쪽, 오른쪽, 뒤로
보상(reward) : 환경내에서 얻을 수 있는 이득, 목표들로 보상을 최대화 해야 함  예) 치즈~ 미로 탈출 등...

정책(Policy) : 상태에 따라 어떤 행동을 해야할지 결정하는 기준, 결국 최적의 정책을 찾도록 학습을 시켜야 함  예) 생쥐 머리속의 정책(어떤 행동을 할지 결정하는 기준)


open ai gym 환경 구경
https://gym.openai.com/envs/#classic_control

2장
http://hunkim.github.io/ml/RL/rl-l02.pdf

python sample

import gym
env = gym.make('FrozenLake-v0')
observation = env.reset()
for _ in range(1000):
  env.render()
  action = env.action_space.sample()  # 현재는 랜덤이지만, 훈련이 되면 올바른 action을 선택하도록 해야 함
  observation, reward, dong, info = env.step(action)



3장
http://hunkim.github.io/ml/RL/rl03.pdf

Q함수 : 입력 (상태, 액션) , 결과 (해당 상태에서 해당 액션을 했을 떄의 향후 기대되는 보상)

Q(state, action) => expected reward
Q함수에게 물어보면 해당 상태에서, 어떤 액션을 수행하는 것이 기대되는 보상을 최대화하는 액션을 선택할 수 있음

Q(s,a) <- reward + max Q(s',a')

Q함수의 업데이트 방법은, 
현재 상태(s)에서 수행한 액션(a)에서 실제로 한스텝 수행 후에 받은 reward 와 다음 스탭에서 수행할 Q 값을 합친 값으로 업데이트 함
==> 한 스탭만 전진하는 방식의 업데이트로 Q값이 정확하게 갱신 되는 걸까?

예시) 
지하철을 타고 강남역에서 삼성역까지 도착시간 예측방법, 액션은 일단 무시하고 상태만으로 생각해보기~
강남역 -> 역삼역 -> 선릉역 -> 삼성역(목적지) 

* 상태값만으로 예측하는 형태는 일반적으로 v(s) 형식으로 표기 함, s는 현재 상태

v(강남역) : 초기 예상치 값을 400초라고 할 경우.....

v(강남역) = 90초 + v(역삼역)
-> 쪼금 더 목적지에 가까워져서 예측하기가 쉬움, 강남의 상태값은 실제 걸린 시간 90초에 v(역삼) 의 합으로 갱신함
-> v(역삼역) 예상치를 300초라고 할떄
-> v(강남역) = 90 + 300 = 390 으로 갱신 (기존 400초 보다는 조금 더 정확해짐)


v(역삼역) = 80초 + v(선릉역)
-> v(선릉역) 예상치를 200 으로 할때
-> v(역삼역) = 80초 + 200 = 280 으로 갱신됨 (기존 300초에서 280으로 변경)

v(선릉역) = 110초  : 선릉역에서 삼성역 도착 시간
-> v(선릉역) = 110초 + v(삼성역) : 목적지(삼성역) 도착이므로 v(삼성역) = 0 
-> v(선릉역) = 110초 + 0 :  실제 걸린 시간으로 갱신

v(선릉역)이 참 값으로 갱신되었으니, 그 앞의 역들의 v(s)값들도 연쇄적으로 참값에 가까워질 수 있음
해당 갱신을 반복하게 되면 결국 참 값으로 갱신하게 됨





